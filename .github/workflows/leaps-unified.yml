name: LEAPS Unified (produce + consume)

on:
  # 10:50 AM local PT (DST-aware, weekdays only)
  # PDT (Marâ€“Oct) 17:50 UTC
  schedule:
    - cron: '50 17 * 3-10 1-5'
    # PST (Novâ€“Feb) 18:50 UTC
    - cron: '50 18 * 11,12,1,2 1-5'
  workflow_dispatch:

permissions:
  contents: write   # commit output + publish gh-pages
  pages: write

env:
  PYTHON_VERSION: "3.11"
  DESIRED_PT_TIME: "10:50"   # target local PT fire time
  WINDOW_MIN: "600"           # Â± minutes allowed around target (keeps single daily run)
  REPO: "Sevenon7/Tradier_Options"
  BRANCH: "main"
  OWNER: "Sevenon7"

jobs:
  # =========================
  # 1) TIME GATE (PT window)
  # =========================
  time-gate:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      run_ok: ${{ steps.tgate.outputs.run_ok }}
    steps:
      - name: Time Gate (America/Los_Angeles)
        id: tgate
        shell: bash
        run: |
          set -euo pipefail
          export TZ=America/Los_Angeles
          NOW_PT="$(date +%H:%M)"
          TARGET_PT="${DESIRED_PT_TIME}"

          # Compute |Î”| minutes between NOW_PT and TARGET_PT using GNU date
          now_s="$(date -d "$NOW_PT" +%s)"
          tgt_s="$(date -d "$TARGET_PT" +%s)"
          diff="$(( now_s - tgt_s ))"; [ "$diff" -lt 0 ] && diff="$(( -diff ))"
          DELTA="$(( diff / 60 ))"

          echo "Now PT: ${NOW_PT}" >> "$GITHUB_STEP_SUMMARY"
          echo "Target PT: ${TARGET_PT}" >> "$GITHUB_STEP_SUMMARY"
          echo "Delta (min): ${DELTA}" >> "$GITHUB_STEP_SUMMARY"
          echo "Window (Â±min): ${WINDOW_MIN}" >> "$GITHUB_STEP_SUMMARY"

          if [ "${DELTA}" -le "${WINDOW_MIN}" ]; then
            echo "run_ok=true"  >> "$GITHUB_OUTPUT"
            echo "âœ… Within window â€” proceeding." >> "$GITHUB_STEP_SUMMARY"
          else
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            echo "â¸ï¸ Outside window â€” skipping execution." >> "$GITHUB_STEP_SUMMARY"
          fi

  # =========================
  # 2) PRODUCER
  # =========================
  producer:
    env:
        PYTHONPATH: ${{ github.workspace }}
    needs: time-gate
    if: needs.time-gate.outputs.run_ok == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 25
    outputs:
      date_dir: ${{ steps.collect.outputs.date_dir }}
      already_ran: ${{ steps.skipcheck.outputs.already }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Ensure OS tools (curl/jq)
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y curl jq coreutils

      - name: Create requirements.txt (deterministic)
        run: |
          set -euo pipefail
          printf '%s\n' \
            'requests' \
            'pandas' \
            'numpy' \
            'python-dateutil' > requirements.txt
          echo "----- requirements.txt -----"
          cat requirements.txt

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Mask token (log hygiene)
        run: echo "::add-mask::${{ secrets.TRADIER_TOKEN }}"

      - name: Skip if already ran today (UTC)
        id: skipcheck
        shell: bash
        run: |
          set -euo pipefail
          DATE_DIR="data/$(date -u +%Y-%m-%d)"
          if [ -d "$DATE_DIR" ]; then
            echo "already=true"  >> "$GITHUB_OUTPUT"
            echo "â­ï¸ Producer already completed for $DATE_DIR" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "already=false" >> "$GITHUB_OUTPUT"
            echo "âœ¨ First run for $DATE_DIR â€” executing Producer." >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Run overlay script (leaps_batched_cached.py)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          set -euo pipefail
          python leaps_batched_cached.py

      - name: Build Actual Option P/L (tools/option_pl_builder.py)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          if [ -f tools/option_pl_builder.py ]; then
            python tools/option_pl_builder.py
          else
            echo "âš ï¸ tools/option_pl_builder.py not present; skipping." >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Enrich overlay with intraday VWAP (optional)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          PYTHONPATH: ${{ github.workspace }}
        continue-on-error: true
        run: |
          if [ -f tools/enrich_overlay_with_vwap.py ]; then
            # Run as a package so "from tools.*" works in any CWD
            if python -m tools.enrich_overlay_with_vwap --overlay overlay_vwap_macd_rsi.csv; then
              echo "âœ… VWAP enrichment completed" >> "$GITHUB_STEP_SUMMARY"
            else
              echo "::warning::VWAP enrichment failed (non-blocking)"
              echo "âš ï¸ VWAP enrichment failed (non-critical)" >> "$GITHUB_STEP_SUMMARY"
            fi
          else
            echo "::warning::tools/enrich_overlay_with_vwap.py not found"
          fi

      - name: Collect artifacts into date dir + update latest.json
        if: steps.skipcheck.outputs.already == 'false'
        id: collect
        shell: bash
        run: |
          set -euo pipefail
          TODAY="$(date -u +%Y-%m-%d)"
          DD="data/${TODAY}"
          mkdir -p "$DD"

          # Move any root-level outputs into the date folder
          for f in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv vwap_missing.json; do
            [ -f "$f" ] && mv -f "$f" "$DD/"
          done

          # Refresh latest.json pointer at repo root
          echo '{"date_dir":"data/'"${TODAY}"'"}' > latest.json

          echo "date_dir=${TODAY}" >> "$GITHUB_OUTPUT"

          {
            echo "### ðŸ“¦ Producer Artifacts"
            echo "- **Directory:** \`${DD}\`"
            echo "- **Contents:**"
            ls -lh "$DD" | tail -n +2 | awk '{printf "  - `%s` (%s)\n", $9, $5}'
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Build analysis_digest.json (root + date_dir)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          DD: ${{ steps.collect.outputs.date_dir }}   # e.g. 2025-10-09
        shell: bash
        run: |
          set -euo pipefail
          ROOT="${GITHUB_WORKSPACE}"
          OVER="data/${DD}/overlay_vwap_macd_rsi.csv"
          PL="data/${DD}/option_pl.csv"
          GAP="data/${DD}/gapdown_above_100sma.csv"

          if [ ! -f "$OVER" ]; then
            echo "::error ::overlay CSV missing at $OVER"
            exit 1
          fi

          python - <<'PY'
          import os, json
          import pandas as pd
          
          dd = os.environ["DD"]                         # 'YYYY-MM-DD'
          over = f"data/{dd}/overlay_vwap_macd_rsi.csv"
          opl  = f"data/{dd}/option_pl.csv"
          gap  = f"data/{dd}/gapdown_above_100sma.csv"
          
          out_path = "analysis_digest.json"
          digest = {"date_dir": f"data/{dd}"}
          
          def safe_rows(path, want_cols=None, head_n=50):
              if not os.path.exists(path):
                  return None
              try:
                  df = pd.read_csv(path)
                  if want_cols:
                      cols = [c for c in want_cols if c in df.columns]
                      df = df[cols]
                  return df.head(head_n).to_dict(orient="records")
              except Exception as e:
                  digest.setdefault("errors", []).append(f"{os.path.basename(path)}: {e}")
                  return None
          
          digest["overlay_preview"] = safe_rows(
              over,
              want_cols=["Ticker","RSI14","MACD>Signal","VWAP","LastPx","Px_vs_VWAP","SMA100","Gap%","Guidance"],
              head_n=30
          )
          
          digest["option_pl_rows"] = 0
          if os.path.exists(opl):
              try:
                  digest["option_pl_rows"] = int(pd.read_csv(opl).shape[0])
              except Exception as e:
                  digest.setdefault("errors", []).append(f"option_pl.csv: {e}")
          
          if os.path.exists(gap):
              try:
                  digest["gap_rows"] = int(pd.read_csv(gap).shape[0])
              except Exception as e:
                  digest.setdefault("errors", []).append(f"gapdown_above_100sma.csv: {e}")
          
          with open(out_path, "w") as f:
              json.dump(digest, f, indent=2)
          print(f"Wrote {out_path}")
          PY

      - name: Commit daily artifacts to repo
        if: steps.skipcheck.outputs.already == 'false'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "ðŸ“Š LEAPS daily overlay - ${{ needs.initialize.outputs.date_dir }}"
          branch: ${{ env.BRANCH }}
          add_options: -A
          commit_options: '--no-verify'
          file_pattern: 'data/* latest.json analysis_digest.json'

      - name: Publish CSV mirror to gh-pages (static backup)
        if: steps.skipcheck.outputs.already == 'false'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: data/${{ steps.collect.outputs.date_dir }}
          destination_dir: data/${{ steps.collect.outputs.date_dir }}
          keep_files: true

  # =========================
  # 3) CONSUMER
  # =========================
  consumer:
    needs: [time-gate, producer]
    if: needs.time-gate.outputs.run_ok == 'true' && !cancelled()
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Ensure OS tools
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y curl jq coreutils

      - name: Make fetch tool executable (repo path)
        run: chmod +x tools/fetch.sh

      - name: Resolve effective date_dir (prefer local latest.json; else newest â‰¤today; else yesterday)
        id: datedir
        shell: bash
        run: |
          set -euo pipefail
          TODAY="$(date -u +%Y-%m-%d)"
          mkdir -p tmp

          # 1) Try local latest.json pointer
          if [ -s latest.json ]; then
            ptr="$(jq -r '.date_dir // empty' latest.json | sed 's|^data/||')"
            if [ -n "$ptr" ]; then
              echo "Using local pointer: $ptr"
              echo "$ptr" > tmp/dd.txt
            fi
          fi

          # 2) Fallback: newest folder â‰¤ today
          if [ ! -s tmp/dd.txt ]; then
            d="$(ls -1d data/20* 2>/dev/null | sed 's|^data/||' | sort | awk -v t="$TODAY" '$0<=t' | tail -1)"
            if [ -n "$d" ]; then
              echo "Using newest existing folder: $d"
              echo "$d" > tmp/dd.txt
            fi
          fi

          # 3) Final fallback: yesterday
          if [ ! -s tmp/dd.txt ]; then
            y="$(date -u -d 'yesterday' +%Y-%m-%d)"
            echo "Falling back to yesterday: $y"
            echo "$y" > tmp/dd.txt
          fi

          echo "date_dir=$(cat tmp/dd.txt)" >> "$GITHUB_OUTPUT"

      - name: Fetch overlay/PL/gap with local-preferred fallback (raw â†’ API â†’ Pages)
        id: fetch_data
        shell: bash
        env:
          OWNER:  ${{ env.OWNER }}
          REPO:   ${{ env.REPO }}
          BRANCH: ${{ env.BRANCH }}
        run: |
          set -euo pipefail
          DD="${{ steps.datedir.outputs.date_dir }}"
          SRC="data/${DD}"
          echo "Using date_dir: ${DD} (SRC=${SRC})"

          mkdir -p tmp
          copied=0

          # 1) Local-first copies
          if [ -f "${SRC}/overlay_vwap_macd_rsi.csv" ]; then
            cp -f "${SRC}/overlay_vwap_macd_rsi.csv" overlay.csv && copied=$((copied+1))
            echo "âœ“ overlay.csv from ${SRC}"
          fi
          if [ -f "${SRC}/option_pl.csv" ]; then
            cp -f "${SRC}/option_pl.csv" option_pl.csv && copied=$((copied+1))
            echo "âœ“ option_pl.csv from ${SRC}"
          fi
          if [ -f "${SRC}/gapdown_above_100sma.csv" ]; then
            cp -f "${SRC}/gapdown_above_100sma.csv" gap.csv && copied=$((copied+1))
            echo "âœ“ gap.csv from ${SRC}"
          fi

          # 2) For any missing file(s), try remote fetch via tools/fetch.sh
          miss=0
          need_overlay=0
          [ -s overlay.csv ]   || need_overlay=1
          need_pl=0
          [ -s option_pl.csv ] || need_pl=1
          need_gap=0
          [ -s gap.csv ]       || need_gap=1

          if [ $need_overlay -eq 1 ] || [ $need_pl -eq 1 ] || [ $need_gap -eq 1 ]; then
            echo "Some files not found locally; trying remoteâ€¦"
            set +e
            if [ $need_overlay -eq 1 ]; then
              tools/fetch.sh "data/${DD}/overlay_vwap_macd_rsi.csv" overlay.csv || miss=$((miss+1))
            fi
            if [ $need_pl -eq 1 ]; then
              tools/fetch.sh "data/${DD}/option_pl.csv" option_pl.csv || miss=$((miss+1))
            fi
            if [ $need_gap -eq 1 ]; then
              tools/fetch.sh "data/${DD}/gapdown_above_100sma.csv" gap.csv || miss=$((miss+1))
            fi
            set -e
          fi

          # 3) Require overlay.csv; the others are optional
          if [ ! -s overlay.csv ]; then
            echo "::error ::overlay.csv missing after local+remote fallback (checked ${SRC} and GitHub)."
            exit 1
          fi

          [ -s option_pl.csv ] || echo "::notice::option_pl.csv missing (optional)"
          [ -s gap.csv ]       || echo "::notice::gapdown_above_100sma.csv missing (optional)"

          {
            echo "### Consumer inputs"
            echo "- date_dir: \`${DD}\`"
            echo "- overlay.csv: $( [ -s overlay.csv ] && echo present || echo missing )"
            echo "- option_pl.csv: $( [ -s option_pl.csv ] && echo present || echo missing )"
            echo "- gap.csv: $( [ -s gap.csv ] && echo present || echo missing )"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Emit quick summary to Job Summary
        run: |
          {
            echo "### ðŸ“Š Consumer Summary"
            echo "- **date_dir:** ${{ steps.datedir.outputs.date_dir }}"
            echo ""
            if [ -f overlay.csv ]; then
              echo "**overlay.csv (head):**"
              head -5 overlay.csv
              echo ""
            else
              echo "âš ï¸ overlay.csv not found"
            fi
            if [ -f option_pl.csv ]; then
              echo "**option_pl.csv (head):**"
              head -5 option_pl.csv
              echo ""
            else
              echo "âš ï¸ option_pl.csv not found"
            fi
            if [ -f gap.csv ]; then
              echo "**gapdown_above_100sma.csv (head):**"
              head -5 gap.csv
              echo ""
            else
              echo "âš ï¸ gapdown_above_100sma.csv not found"
            fi
          } >> "$GITHUB_STEP_SUMMARY"
