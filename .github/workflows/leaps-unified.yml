name: LEAPS Unified (produce + consume)

on:
  schedule:
    # Single UTC schedule (covers PDT/PST); time-gate constrains to 10:50 PT Â± WINDOW_MIN on weekdays
    - cron: "50 17,18 * * 1-5"
  workflow_dispatch:
    inputs:
      skip_time_gate:
        description: "Skip time gate (manual runs only)"
        required: false
        default: "false"
        type: choice
        options: ["false","true"]
      force_run:
        description: "Force producer even if today's data exists"
        required: false
        default: "false"
        type: choice
        options: ["false","true"]
      target_date:
        description: "Target YYYY-MM-DD (optional)"
        required: false
        type: string

concurrency:
  group: leaps-unified-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

permissions:
  contents: read

env:
  PYTHON_VERSION: "3.11"
  DESIRED_PT_TIME: "10:50"     # Local PT target time
  WINDOW_MIN: "95"             # Â± minutes guard window
  REPO: "Sevenon7/Tradier_Options"
  BRANCH: "main"
  OWNER: "Sevenon7"
  DATA_RETENTION_DAYS: "30"

defaults:
  run:
    shell: bash

jobs:
  # =========================
  # 1) TIME GATE / INIT
  # =========================
  init-time-gate:
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    outputs:
      run_ok: ${{ steps.tgate.outputs.run_ok }}
      date_dir: ${{ steps.setdate.outputs.date_dir }}
    steps:
      - name: Validate required secrets
        run: |
          if [[ -z "${{ secrets.TRADIER_TOKEN }}" ]]; then
            echo "::error::TRADIER_TOKEN secret is not configured"; exit 1; fi
          if [[ -z "${{ secrets.GITHUB_TOKEN }}" ]]; then
            echo "::error::GITHUB_TOKEN is not available"; exit 1; fi

      - name: Mask tokens early
        run: |
          echo "::add-mask::${{ secrets.TRADIER_TOKEN }}"
          echo "::add-mask::${{ secrets.GITHUB_TOKEN }}"

      - name: Resolve target date (UTC)
        id: setdate
        env:
          INPUT_DATE: ${{ github.event.inputs.target_date }}
        run: |
          set -euo pipefail
          if [[ -n "${INPUT_DATE:-}" ]]; then
            if ! date -d "${INPUT_DATE}" +%Y-%m-%d >/dev/null 2>&1; then
              echo "::error::Invalid date format: ${INPUT_DATE}"; exit 1
            fi
            DD="${INPUT_DATE}"
          else
            DD="$(date -u +%Y-%m-%d)"
          fi
          echo "date_dir=${DD}" >> "$GITHUB_OUTPUT"
          cat >> "$GITHUB_STEP_SUMMARY" <<'MD'
### ðŸš€ Workflow Initialization - Target date resolved.
MD

      - name: Time Gate (America/Los_Angeles)
        id: tgate
        env:
          SKIP_TG: ${{ github.event.inputs.skip_time_gate || 'false' }}
          DESIRED_PT_TIME: ${{ env.DESIRED_PT_TIME }}
          WINDOW_MIN: ${{ env.WINDOW_MIN }}
        run: |
          set -euo pipefail
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${SKIP_TG}" == "true" ]]; then
            echo "run_ok=true" >> "$GITHUB_OUTPUT"
            cat >> "$GITHUB_STEP_SUMMARY" <<'MD'
### â­ï¸ Time gate skipped (manual override)
MD
            exit 0
          fi

          export TZ=America/Los_Angeles
          NOW_PT="$(date +%H:%M)"
          DOW="$(date +%u)"  # 1..7 Mon..Sun

          if [[ "$DOW" -gt 5 ]]; then
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            cat >> "$GITHUB_STEP_SUMMARY" <<'MD'
### â° Time Gate: BLOCKED (weekend)
MD
            exit 0
          fi

          now_s="$(date -d "$NOW_PT" +%s)"
          tgt_s="$(date -d "$DESIRED_PT_TIME" +%s)"
          diff="$(( now_s - tgt_s ))"; [[ $diff -lt 0 ]] && diff="$(( -diff ))"
          delta="$(( diff / 60 ))"

          if [[ "$delta" -le "$WINDOW_MIN" ]]; then
            echo "run_ok=true" >> "$GITHUB_OUTPUT"
            cat >> "$GITHUB_STEP_SUMMARY" <<'MD'
### â° Time Gate: WITHIN WINDOW â€” proceeding
MD
          else
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            cat >> "$GITHUB_STEP_SUMMARY" <<'MD'
### â° Time Gate: OUTSIDE WINDOW â€” skipping
MD
          fi

  # =========================
  # 2) PRODUCER
  # =========================
  producer:
    needs: init-time-gate
    if: needs.init-time-gate.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    timeout-minutes: 30
    permissions:
      contents: write
      pages: write
    outputs:
      produced_date: ${{ steps.collect.outputs.date_dir }}
      already: ${{ steps.skip.outputs.already }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Install system deps
        run: |
          set -euo pipefail
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends curl jq coreutils
          sudo apt-get clean && sudo rm -rf /var/lib/apt/lists/*

      - name: Setup Python ${{ env.PYTHON_VERSION }}
        uses: actions/setup-python@v5
        with: { python-version: ${{ env.PYTHON_VERSION }} }

      - name: Create requirements.txt (deterministic)
        run: |
          printf '%s\n' \
            'requests==2.32.3' \
            'pandas==2.2.2' \
            'numpy==1.26.4' \
            'python-dateutil==2.9.0.post0' > requirements.txt

      - name: Install Python deps (deterministic)
        env:
          PIP_DISABLE_PIP_VERSION_CHECK: "1"
          PIP_NO_INPUT: "1"
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      - name: Check if producer already ran
        id: skip
        env:
          DD: ${{ needs.init-time-gate.outputs.date_dir }}
          FORCE_RUN: ${{ github.event.inputs.force_run || 'false' }}
        run: |
          set -euo pipefail
          if [[ "${FORCE_RUN}" == "true" ]]; then
            echo "already=false" >> "$GITHUB_OUTPUT"
            cat >> "$GITHUB_STEP_SUMMARY" <<'MD'
### âœ¨ Producer: force run enabled (ignoring existing data)
MD
            exit 0
          fi

          if [[ -d "data/${DD}" && -s "data/${DD}/overlay_vwap_macd_rsi.csv" ]]; then
            echo "already=true" >> "$GITHUB_OUTPUT"
            {
              echo "### â­ï¸ Producer: SKIPPED (data exists)"
              echo "- dir: data/${DD}"
            } >> "$GITHUB_STEP_SUMMARY"
          else
            echo "already=false" >> "$GITHUB_OUTPUT"
            {
              echo "### âœ¨ Producer: RUNNING"
              echo "- target dir: data/${DD}"
            } >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Run main LEAPS script
        if: steps.skip.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          set -euo pipefail
          python leaps_batched_cached.py

      - name: Build Option P/L Analysis
        if: steps.skip.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          set -euo pipefail
          if [[ -f tools/option_pl_builder.py ]]; then
            python tools/option_pl_builder.py || echo "::warning::option_pl_builder.py failed (non-fatal)"
          fi

      - name: Enrich with VWAP data (imports tools/*)
        if: steps.skip.outputs.already == 'false'
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          set -euo pipefail
          if [[ -f tools/enrich_overlay_with_vwap.py ]]; then
            python tools/enrich_overlay_with_vwap.py --overlay overlay_vwap_macd_rsi.csv || \
              echo "::warning::VWAP enrichment failed (non-fatal)"
          fi

      - name: Validate produced files (soft)
        if: steps.skip.outputs.already == 'false'
        continue-on-error: true
        run: |
          python - <<'PY'
          import os, pandas as pd
          errs=[]
          if not os.path.exists('overlay_vwap_macd_rsi.csv'): errs.append('overlay_vwap_macd_rsi.csv missing')
          else:
              try: pd.read_csv('overlay_vwap_macd_rsi.csv')
              except Exception as e: errs.append(f'overlay read error: {e}')
          for f in ['option_pl.csv','gapdown_above_100sma.csv']:
              if os.path.exists(f):
                  try: pd.read_csv(f)
                  except Exception as e: print(f'::warning::{f} read warn: {e}')
          [print(f'::error::{e}') for e in errs]
          sys.exit(1 if errs else 0)
          PY

      - name: Move artifacts into date dir (if new)
        if: steps.skip.outputs.already == 'false'
        id: collect
        env:
          TODAY_UTC: ${{ needs.init-time-gate.outputs.date_dir }}
        run: |
          set -euo pipefail
          DEST="data/${TODAY_UTC}"
          mkdir -p "${DEST}"
          for f in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv vwap_missing.json; do
            [[ -f "$f" ]] && mv -f "$f" "${DEST}/"
          done
          echo "date_dir=${TODAY_UTC}" >> "$GITHUB_OUTPUT"

      - name: Build/refresh latest.json & analysis_digest.json
        if: steps.skip.outputs.already == 'false'
        env:
          TODAY_UTC: ${{ needs.init-time-gate.outputs.date_dir }}
          RUN_ID: ${{ github.run_id }}
        run: |
          set -euo pipefail
          # latest.json
          jq -n --arg dd "${TODAY_UTC}" --arg ts "$(date -u +%Y-%m-%dT%H:%M:%SZ)" --arg run "$RUN_ID" \
            '{date_dir:("data/"+$dd),generated_utc:$ts,run_id:$run}' > latest.json

          # analysis_digest.json
          python - <<'PY'
          import os, json, pandas as pd
          from datetime import datetime
          dd = os.environ["TODAY_UTC"]
          base = f"data/{dd}"
          out = {"date_dir": base, "generated_utc": datetime.utcnow().isoformat()+"Z", "files":{}}
          def grab(p, cols=None):
              if not os.path.exists(p): return {"status":"missing"}
              try:
                  df = pd.read_csv(p)
                  d = {"status":"ok","rows":len(df),"columns":len(df.columns)}
                  if cols: d["preview"]=df[[c for c in cols if c in df.columns]].head(30).to_dict("records")
                  return d
              except Exception as e:
                  return {"status":"error","error":str(e)}
          out["files"]["overlay"]=grab(f"{base}/overlay_vwap_macd_rsi.csv",
              ["Ticker","RSI14","MACD>Signal","VWAP","LastPx","Px_vs_VWAP","SMA100","Gap%","Guidance"])
          out["files"]["option_pl"]=grab(f"{base}/option_pl.csv")
          out["files"]["gap"]=grab(f"{base}/gapdown_above_100sma.csv")
          with open("analysis_digest.json","w") as f: json.dump(out,f,indent=2)
          PY

      - name: Commit artifacts (metadata always; data only when new)
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "LEAPS: artifacts for ${{ steps.collect.outputs.date_dir }}"
          branch: ${{ env.BRANCH }}
          add_options: "-A"
          file_pattern: |
            data/${{ steps.collect.outputs.date_dir }}/*
            latest.json
            analysis_digest.json

      - name: Prepare GitHub Pages payload
        if: steps.skip.outputs.already == 'false'
        run: |
          set -euo pipefail
          DD="${{ steps.collect.outputs.date_dir }}"
          rm -rf pages_pub
          mkdir -p pages_pub/data/"${DD}"
          cp -a "data/${DD}/." "pages_pub/data/${DD}/"
          cp -a latest.json analysis_digest.json pages_pub/

      - name: Deploy to GitHub Pages
        if: steps.skip.outputs.already == 'false'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: pages_pub
          keep_files: true
          enable_jekyll: false

      - name: Cleanup old data directories
        if: steps.skip.outputs.already == 'false'
        continue-on-error: true
        run: |
          set -euo pipefail
          [[ -d data ]] && find data -maxdepth 1 -type d -name "20*" -mtime +${DATA_RETENTION_DAYS} -print -exec rm -rf {} + || true

  # =========================
  # 3) CONSUMER
  # =========================
  consumer:
    needs: [init-time-gate, producer]
    if: needs.init-time-gate.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    timeout-minutes: 12
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install system deps
        run: |
          set -euo pipefail
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends curl jq coreutils python3
          sudo apt-get clean && sudo rm -rf /var/lib/apt/lists/*

      - name: Make fetch tool executable
        run: |
          if [[ -f tools/fetch.sh ]]; then chmod +x tools/fetch.sh; else echo "::error::tools/fetch.sh missing"; exit 1; fi

      - name: Resolve effective date dir (prefer local latest.json; else newest â‰¤ today; else yesterday)
        id: datedir
        run: |
          set -euo pipefail
          TODAY="$(date -u +%Y-%m-%d)"; CAND=""
          if [[ -s latest.json ]]; then
            ptr="$(jq -r '.date_dir // empty' latest.json | sed 's|^data/||')"
            if [[ "$ptr" =~ ^[0-9]{4}-[0-9]{2}-[0-9]{2}$ ]]; then
              python3 - <<PY > tmp_dd.txt
              import os,sys,datetime
              t=datetime.datetime.strptime(os.environ["TODAY"],"%Y-%m-%d")
              p=datetime.datetime.strptime("${ptr}","%Y-%m-%d")
              print("${ptr}" if 0 <= (t-p).total_seconds()/3600.0 <= 24 else "")
              PY
              CAND="$(cat tmp_dd.txt)"; rm -f tmp_dd.txt || true
            fi
          fi
          if [[ -z "$CAND" && -d data ]]; then
            CAND="$(find data -maxdepth 1 -type d -name '20*' -printf '%f\n' | sort | tail -1 || true)"
          fi
          [[ -z "$CAND" ]] && CAND="$(date -u -d 'yesterday' +%Y-%m-%d)"
          echo "date_dir=${CAND}" >> "$GITHUB_OUTPUT"

      - name: Fetch overlay/PL/gap with fallback chain (Pages â†’ jsDelivr â†’ raw â†’ API)
        env:
          DD: ${{ steps.datedir.outputs.date_dir }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OWNER: ${{ env.OWNER }}
          REPO:  ${{ env.REPO }}
          BRANCH: ${{ env.BRANCH }}
        run: |
          set -euo pipefail
          found=0
          for p in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv; do
            if [[ -s "data/${DD}/${p}" ]]; then cp "data/${DD}/${p}" "$(basename $p .csv).csv"; found=$((found+1)); fi
          done
          if [[ $found -eq 0 ]]; then
            tools/fetch.sh \
              "data/${DD}/overlay_vwap_macd_rsi.csv" overlay.csv \
              "data/${DD}/option_pl.csv"            option_pl.csv \
              "data/${DD}/gapdown_above_100sma.csv" gap.csv
          fi

      - name: Emit quick summary to Job Summary
        run: |
          {
            echo "### ðŸ“Š Data Summary â€” ${DD}"
            for f in overlay.csv option_pl.csv gap.csv; do
              if [[ -s "$f" ]]; then
                echo "#### $f"; echo '```csv'; head -5 "$f"; echo '```'
              fi
            done
          } >> "$GITHUB_STEP_SUMMARY"

  # =========================
  # 4) NOTIFY
  # =========================
  notify:
    needs: [init-time-gate, producer, consumer]
    if: always()
    runs-on: ubuntu-24.04
    steps:
      - name: Final status
        run: |
          {
            echo "## ðŸ“‹ Workflow Execution Summary"
            echo "| Job | Status |"
            echo "|---|---|"
            echo "| Time Gate | ${{ needs.init-time-gate.result }} |"
            echo "| Producer | ${{ needs.producer.result }} |"
            echo "| Consumer | ${{ needs.consumer.result }} |"
          } >> "$GITHUB_STEP_SUMMARY"
