name: LEAPS Unified (produce + consume)

on:
  # DST-safe: trigger twice; time-gate lets only the in-window run proceed.
  schedule:
    - cron: "50 17,18 * * 1-5"
  workflow_dispatch:
    inputs:
      skip_time_gate:
        type: boolean
        description: "Skip time gate"
        default: false
      force_run:
        type: boolean
        description: "Force producer even if today's data exists"
        default: false
      target_date_pt:
        type: string
        description: "Override PT date for data directory (YYYY-MM-DD, optional)"

concurrency:
  group: leaps-unified-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: write
  pages: write

env:
  PYTHON_VERSION: "3.11"
  DESIRED_PT_TIME: "10:50"   # local PT
  WINDOW_MIN: "95"
  OWNER: "Sevenon7"
  REPO: "Sevenon7/Tradier_Options"
  BRANCH: "main"
  DATA_RETENTION_DAYS: "30"

defaults:
  run:
    shell: bash

jobs:
  # ------------------------------
  # 1) Time gate / init
  # ------------------------------
  init-time-gate:
    runs-on: ubuntu-24.04
    outputs:
      run_ok: ${{ steps.tgate.outputs.run_ok }}
      date_pt: ${{ steps.date.outputs.date_pt }}
    steps:
      - name: Secrets & masks
        run: |
          if [[ -z "${{ secrets.TRADIER_TOKEN }}" ]]; then
            echo "::error::TRADIER_TOKEN is not configured"; exit 1; fi
          echo "::add-mask::${{ secrets.TRADIER_TOKEN }}"

      - name: Resolve PT date (consistent folder naming)
        id: date
        run: |
          set -euo pipefail
          if [[ -n "${{ github.event.inputs.target_date_pt || '' }}" ]]; then
            dd="${{ github.event.inputs.target_date_pt }}"
            date -d "$dd" +%Y-%m-%d >/dev/null 2>&1 || { echo "::error::Bad PT date"; exit 1; }
          else
            dd="$(TZ=America/Los_Angeles date +%Y-%m-%d)"
          fi
          echo "date_pt=$dd" >> "$GITHUB_OUTPUT"
          echo "PT date: $dd" >> "$GITHUB_STEP_SUMMARY"

      - name: Time gate (America/Los_Angeles)
        id: tgate
        run: |
          set -euo pipefail
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.skip_time_gate || 'false' }}" == "true" ]]; then
            echo "run_ok=true" >> "$GITHUB_OUTPUT"
            echo "â­ï¸ Time gate skipped (manual override)" >> "$GITHUB_STEP_SUMMARY"
            exit 0
          fi
          export TZ=America/Los_Angeles
          now="$(date +%H:%M)"
          tgt="${DESIRED_PT_TIME}"
          ns="$(date -d "$now" +%s)"
          ts="$(date -d "$tgt" +%s)"
          diff=$(( ns - ts )); (( diff < 0 )) && diff=$(( -diff ))
          delta=$(( diff / 60 ))
          echo "Now PT: $now | Target: $tgt | Î”=${delta}m" >> "$GITHUB_STEP_SUMMARY"
          if (( delta <= WINDOW_MIN )); then
            echo "run_ok=true"  >> "$GITHUB_OUTPUT"
            echo "âœ… Within window" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            echo "â¸ï¸ Outside window" >> "$GITHUB_STEP_SUMMARY"
          fi

  # ------------------------------
  # 2) Producer
  # ------------------------------
  producer:
    needs: init-time-gate
    if: needs.init-time-gate.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    outputs:
      produced_dir: ${{ steps.move.outputs.dir }}
      already: ${{ steps.skip.outputs.already }}
    steps:
      - uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - uses: actions/setup-python@v5
        with: { python-version: ${{ env.PYTHON_VERSION }} }

      - name: Install system deps
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends curl jq coreutils
          sudo apt-get clean
          sudo rm -rf /var/lib/apt/lists/*

      - name: Pin requirements
        run: |
          printf '%s\n' \
            'requests==2.32.3' \
            'pandas==2.2.2' \
            'numpy==1.26.4' \
            'python-dateutil==2.9.0.post0' > requirements.txt
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      - name: Check if producer already ran (by PT date)
        id: skip
        run: |
          set -euo pipefail
          DD="${{ needs.init-time-gate.outputs.date_pt }}"
          if [[ "${{ github.event.inputs.force_run || 'false' }}" == "true" ]]; then
            echo "already=false" >> "$GITHUB_OUTPUT"; exit 0; fi
          if [[ -s "data/$DD/overlay_vwap_macd_rsi.csv" ]]; then
            echo "already=true" >> "$GITHUB_OUTPUT"
            echo "Data exists at data/$DD" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "already=false" >> "$GITHUB_OUTPUT"
          fi

      - name: Run main script
        if: steps.skip.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          python leaps_batched_cached.py

      - name: Build Option P/L (best-effort)
        if: steps.skip.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          if [[ -f tools/option_pl_builder.py ]]; then
            python tools/option_pl_builder.py || echo "::warning::option_pl_builder failed"
          fi

      - name: Enrich with VWAP (imports tools.*)
        if: steps.skip.outputs.already == 'false'
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          if [[ -f tools/enrich_overlay_with_vwap.py ]]; then
            python tools/enrich_overlay_with_vwap.py --overlay overlay_vwap_macd_rsi.csv || \
              echo "::warning::VWAP enrichment failed"
          fi

      - name: Move artifacts into date dir (PT)
        if: steps.skip.outputs.already == 'false'
        id: move
        run: |
          set -euo pipefail
          DD="${{ needs.init-time-gate.outputs.date_pt }}"
          DEST="data/$DD"; mkdir -p "$DEST"
          for f in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv vwap_missing.json; do
            [[ -f "$f" ]] && mv -f "$f" "$DEST/"
          done
          echo "dir=$DD" >> "$GITHUB_OUTPUT"

      - name: Commit data (new runs only)
        if: steps.skip.outputs.already == 'false'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "LEAPS: artifacts for ${{ steps.move.outputs.dir }}"
          file_pattern: data/${{ steps.move.outputs.dir }}/*

      - name: Retention cleanup (best-effort)
        if: steps.skip.outputs.already == 'false'
        run: |
          [[ -d data ]] && find data -maxdepth 1 -type d -name "20*" -mtime +${DATA_RETENTION_DAYS} -print -exec rm -rf {} + || true

  # ------------------------------
  # 3) Publisher (always rebuild pointer & digest)
  # ------------------------------
  publish:
    needs: [init-time-gate, producer]
    if: needs.init-time-gate.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    outputs:
      effective_dir: ${{ steps.resolve.outputs.dir }}
    steps:
      - uses: actions/checkout@v4

      - name: Resolve effective date_dir from existing data
        id: resolve
        run: |
          set -euo pipefail
          CAND="${{ needs.producer.outputs.produced_dir }}"
          if [[ -n "$CAND" && -s "data/$CAND/overlay_vwap_macd_rsi.csv" ]]; then
            echo "dir=$CAND" >> "$GITHUB_OUTPUT"; exit 0; fi
          D=$(find data -maxdepth 1 -type d -name "20*" -printf '%f\n' | sort | \
               while read d; do [[ -s "data/$d/overlay_vwap_macd_rsi.csv" ]] && echo "$d"; done | tail -1)
          if [[ -z "$D" ]]; then
            echo "::error::No usable data directory found"; exit 1; fi
          echo "dir=$D" >> "$GITHUB_OUTPUT"
          echo "Using data/$D" >> "$GITHUB_STEP_SUMMARY"

      - name: Build latest.json (points to an existing dir)
        run: |
          dd="${{ steps.resolve.outputs.dir }}"
          jq -n --arg dd "$dd" --arg ts "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            '{date_dir: ("data/" + $dd), generated_utc: $ts}' > latest.json

      - name: Build analysis_digest.json
        run: |
          python - <<'PY'
          import os, json, pandas as pd
          dd = os.environ["DD"]
          base = f"data/{dd}"
          out = {"date_dir": f"data/{dd}", "files": {}}
          def meta(path, preview_cols=None, n=30):
            if not os.path.exists(path): return {"status":"missing"}
            try:
              df = pd.read_csv(path)
              info = {"status":"ok","rows":int(df.shape[0]),"columns":int(df.shape[1])}
              if preview_cols:
                cols=[c for c in preview_cols if c in df.columns]
                if cols: info["preview"]=df[cols].head(n).to_dict("records")
              return info
            except Exception as e:
              return {"status":"error","error":str(e)}
          over = os.path.join(base,"overlay_vwap_macd_rsi.csv")
          opl  = os.path.join(base,"option_pl.csv")
          gap  = os.path.join(base,"gapdown_above_100sma.csv")
          out["files"]["overlay"] = meta(over, ["Ticker","RSI14","MACD>Signal","VWAP","LastPx","Px_vs_VWAP","SMA100","Gap%","Guidance"])
          out["files"]["option_pl"] = meta(opl)
          out["files"]["gap"] = meta(gap)
          with open("analysis_digest.json","w") as f: json.dump(out,f,indent=2)
          print("Wrote analysis_digest.json for", dd)
          PY
        env:
          DD: ${{ steps.resolve.outputs.dir }}

      - name: Commit pointer & digest
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "LEAPS: refresh pointers & artifacts"
          file_pattern: |
            latest.json
            analysis_digest.json

      - name: Publish to GitHub Pages (include pointers & the dayâ€™s folder)
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: .
          destination_dir: .
          keep_files: true
          enable_jekyll: false

  # ------------------------------
  # 4) Consumer (reads latest.json first)
  # ------------------------------
  consumer:
    needs: [init-time-gate, publish]
    if: needs.init-time-gate.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    steps:
      - uses: actions/checkout@v4

      - name: Make fetch tool executable (if present)
        run: |
          [[ -f tools/fetch.sh ]] && chmod +x tools/fetch.sh || true

      - name: Resolve date_dir for read
        id: dd
        run: |
          set -euo pipefail
          if [[ -s latest.json ]]; then
            ptr="$(jq -r '.date_dir // empty' latest.json | sed 's|^data/||')"
          fi
          if [[ -n "$ptr" && -s "data/$ptr/overlay_vwap_macd_rsi.csv" ]]; then
            echo "dir=$ptr" >> "$GITHUB_OUTPUT"; exit 0; fi
          D=$(find data -maxdepth 1 -type d -name "20*" -printf '%f\n' | sort | \
               while read d; do [[ -s "data/$d/overlay_vwap_macd_rsi.csv" ]] && echo "$d"; done | tail -1)
          [[ -n "$D" ]] || { echo "::error::no data"; exit 1; }
          echo "dir=$D" >> "$GITHUB_OUTPUT"

      - name: Show first rows
        run: |
          DD="${{ steps.dd.outputs.dir }}"
          for f in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv; do
            p="data/$DD/$f"
            if [[ -s "$p" ]]; then
              echo "### $p"; head -5 "$p"; echo
            fi
          done

  # ------------------------------
  # 5) Notify (summary + optional alerts)
  # ------------------------------
  notify:
    needs: [init-time-gate, producer, publish, consumer]
    if: always()
    runs-on: ubuntu-24.04
    permissions:
      contents: read
      issues: write
    steps:
      - name: Summarize run
        id: sum
        run: |
          set -euo pipefail
          tg="${{ needs.init-time-gate.result }}"
          pr="${{ needs.producer.result }}"
          pb="${{ needs.publish.result }}"
          cs="${{ needs.consumer.result }}"
          echo "tg=$tg" >> $GITHUB_OUTPUT
          echo "pr=$pr" >> $GITHUB_OUTPUT
          echo "pb=$pb" >> $GITHUB_OUTPUT
          echo "cs=$cs" >> $GITHUB_OUTPUT

          overall="success"
          for s in "$tg" "$pr" "$pb" "$cs"; do
            [[ "$s" == "failure" ]] && overall="failure"
          done
          echo "overall=$overall" >> $GITHUB_OUTPUT

          {
            echo "## ðŸ“‹ Workflow Summary"
            echo ""
            echo "| Job | Status |"
            echo "|-----|--------|"
            echo "| init-time-gate | $tg |"
            echo "| producer       | $pr |"
            echo "| publish        | $pb |"
            echo "| consumer       | $cs |"
            echo ""
            echo "**Overall:** ${overall^^}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Create failure issue (scheduled runs only)
        if: steps.sum.outputs.overall == 'failure' && github.event_name == 'schedule'
        uses: actions/github-script@v7
        with:
          script: |
            const title = `LEAPS Unified failure: run ${context.runId}`;
            const body = [
              `Run: ${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`,
              `Workflow: ${context.workflow}`,
              `Branch: ${context.ref}`,
              `Jobs:`,
              `- init-time-gate: ${'${{ steps.sum.outputs.tg }}'}`,
              `- producer: ${'${{ steps.sum.outputs.pr }}'}`,
              `- publish: ${'${{ steps.sum.outputs.pb }}'}`,
              `- consumer: ${'${{ steps.sum.outputs.cs }}'}`,
            ].join('\n');
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title,
              body,
              labels: ['workflow-failure','automated']
            });

      - name: Slack notify (optional)
        if: always() && env.SLACK_WEBHOOK_URL != ''
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        run: |
          status="${{ steps.sum.outputs.overall }}"
          payload=$(jq -n --arg text "LEAPS Unified: *$status* (run ${{ github.run_id }})" '{text:$text}')
          curl -sSf -X POST -H 'Content-Type: application/json' \
            --data "$payload" "$SLACK_WEBHOOK_URL" >/dev/null || echo "::warning::Slack notify failed"
