name: LEAPS Unified (produce + consume) â€” v1.8.0

on:
  # Single UTC cron; time-gate enforces 12:35 PT Â± 60m for both PDT & PST
  schedule:
    - cron: '5 20 * * 1-5'   # 20:05 UTC Monâ€“Fri
  workflow_dispatch:
    inputs:
      skip_time_gate:
        description: 'Skip time gate (manual runs only)'
        required: false
        type: boolean
        default: false
      debug_mode:
        description: 'Verbose logs'
        required: false
        type: boolean
        default: false
      force_run:
        description: 'Force producer even if today already exists'
        required: false
        type: boolean
        default: false
      target_date:
        description: 'Override date_dir (YYYY-MM-DD)'
        required: false
        type: string

permissions:
  contents: write
  pages: write
  id-token: write

concurrency:
  group: leaps-unified-${{ github.ref }}
  cancel-in-progress: false

env:
  OWNER: "Sevenon7"
  REPO: "Sevenon7/Tradier_Options"
  BRANCH: "main"

  PYTHON_VERSION: "3.11"
  # Time-gate target & window (covers DST with single cron)
  DESIRED_PT_TIME: "12:35"
  WINDOW_MIN: "60"

  # Ensure tools/*.py modules import cleanly (VWAP enrichment)
  PYTHONPATH: ${{ github.workspace }}

  DATA_RETENTION_DAYS: "30"
  MAX_RETRY_ATTEMPTS: "3"

jobs:
  initialize:
    name: Init / Time-gate
    runs-on: ubuntu-latest
    timeout-minutes: 6
    outputs:
      run_ok: ${{ steps.tgate.outputs.run_ok }}
      date_dir: ${{ steps.datepick.outputs.date_dir }}
      cache_key: ${{ steps.cachekey.outputs.key }}
    steps:
      - name: Mask tokens early
        run: |
          echo "::add-mask::${{ secrets.TRADIER_TOKEN }}"
          echo "::add-mask::${{ secrets.GITHUB_TOKEN }}"

      - name: Resolve date_dir
        id: datepick
        shell: bash
        run: |
          set -euo pipefail
          if [[ -n "${{ github.event.inputs.target_date || '' }}" ]]; then
            DD="${{ github.event.inputs.target_date }}"
          else
            DD="$(date -u +%Y-%m-%d)"
          fi
          echo "date_dir=${DD}" >> "$GITHUB_OUTPUT"
          echo "ðŸ“… date_dir=${DD}" >> "$GITHUB_STEP_SUMMARY"

      - name: Time-gate (America/Los_Angeles)
        id: tgate
        shell: bash
        run: |
          set -euo pipefail
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.skip_time_gate }}" == "true" ]]; then
            echo "run_ok=true" >> "$GITHUB_OUTPUT"
            echo "â­ï¸ Time-gate skipped (manual override)" >> "$GITHUB_STEP_SUMMARY"
            exit 0
          fi

          export TZ=America/Los_Angeles
          NOW_PT="$(date +%H:%M)"
          TGT="${DESIRED_PT_TIME}"
          DOW="$(date +%u)"
          if [[ "$DOW" -gt 5 ]]; then
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            echo "ðŸš« Weekend â€” skip" >> "$GITHUB_STEP_SUMMARY"
            exit 0
          fi

          ns="$(date -d "$NOW_PT" +%s)"
          ts="$(date -d "$TGT" +%s)"
          diff=$(( ns - ts ))
          (( diff < 0 )) && diff=$(( -diff ))
          delta=$(( diff / 60 ))

          if (( delta <= WINDOW_MIN )); then
            echo "run_ok=true" >> "$GITHUB_OUTPUT"
            echo "âœ… Within window: $NOW_PT vs $TGT (Î” ${delta}m)" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            echo "â¸ï¸ Outside window: $NOW_PT vs $TGT (Î” ${delta}m)" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Cache key (weekly + req hash)
        id: cachekey
        run: |
          WEEK=$(date -u +%Y-%W)
          echo "key=deps-${WEEK}-${{ hashFiles('**/requirements.txt') }}" >> "$GITHUB_OUTPUT"

  producer:
    needs: initialize
    if: needs.initialize.outputs.run_ok == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 35
    outputs:
      already: ${{ steps.skipcheck.outputs.already }}
      date_dir: ${{ needs.initialize.outputs.date_dir }}
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ needs.initialize.outputs.cache_key }}
          restore-keys: deps-

      - name: OS deps (curl/jq)
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends curl jq coreutils
          sudo rm -rf /var/lib/apt/lists/*

      - name: Install Python deps (deterministic)
        run: |
          set -euo pipefail
          cat > requirements.txt <<'REQ'
          requests==2.31.0
          pandas==2.1.4
          numpy==1.26.2
          python-dateutil==2.8.2
          REQ
          python -m pip install --upgrade pip wheel setuptools
          pip install -r requirements.txt
          [[ "${{ github.event.inputs.debug_mode || 'false' }}" == "true" ]] && pip list

      - name: Skip if data for today already present
        id: skipcheck
        shell: bash
        env:
          DD: ${{ needs.initialize.outputs.date_dir }}
        run: |
          set -euo pipefail
          if [[ "${{ github.event.inputs.force_run || 'false' }}" == "true" ]]; then
            echo "already=false" >> "$GITHUB_OUTPUT"
            echo "ðŸ”„ Force run â€” ignoring existing artifacts" >> "$GITHUB_STEP_SUMMARY"
            exit 0
          fi
          if [[ -f "data/${DD}/overlay_vwap_macd_rsi.csv" ]]; then
            echo "already=true" >> "$GITHUB_OUTPUT"
            echo "â­ï¸ Already produced: data/${DD}" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "already=false" >> "$GITHUB_OUTPUT"
            echo "âœ¨ Producer will run for data/${DD}" >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Run main LEAPS script
        if: steps.skipcheck.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        shell: bash
        run: |
          set -euo pipefail
          a=1; max=${MAX_RETRY_ATTEMPTS:-3}
          until python leaps_batched_cached.py; do
            ec=$?
            [[ $a -ge $max ]] && exit $ec
            echo "Retry $a/$max after 30sâ€¦"; sleep 30; a=$((a+1))
          done

      - name: Build Actual Option P/L (optional)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          if [[ -f tools/option_pl_builder.py ]]; then
            python tools/option_pl_builder.py || echo "::warning::option_pl_builder failed (continuing)"
          fi

      - name: Enrich overlay with intraday VWAP (PYTHONPATH set)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          PYTHONPATH: ${{ github.workspace }}
        run: |
          if [[ -f tools/enrich_overlay_with_vwap.py ]]; then
            python tools/enrich_overlay_with_vwap.py --overlay overlay_vwap_macd_rsi.csv || echo "::warning::VWAP enrich failed (continuing)"
          fi

      - name: Validate produced data (light)
        id: validate
        continue-on-error: true
        run: |
          python - <<'PY'
          import os, json, pandas as pd
          ok=True; errs=[]
          if os.path.exists('overlay_vwap_macd_rsi.csv'):
              try: pd.read_csv('overlay_vwap_macd_rsi.csv', nrows=5)
              except Exception as e: ok=False; errs.append(f'overlay: {e}')
          else:
              ok=False; errs.append('overlay missing')
          print(f"valid={ok}")
          open(os.environ['GITHUB_OUTPUT'],'a').write(f"valid={'true' if ok else 'false'}\n")
          if not ok:
              print("::warning::" + "; ".join(errs))
          PY

      - name: Collect into date dir + write latest.json + analysis_digest.json
        if: steps.skipcheck.outputs.already == 'false'
        env:
          TODAY: ${{ needs.initialize.outputs.date_dir }}
        shell: bash
        run: |
          set -euo pipefail
          DD="data/${TODAY}"
          mkdir -p "$DD"

          # Move any root CSVs (old layout safety)
          for f in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv vwap_missing.json; do
            [[ -f "$f" ]] && mv -f "$f" "$DD/"
          done

          # Write latest.json (root)
          jq -n --arg dd "$TODAY" --arg ts "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            '{date_dir: ("data/" + $dd), timestamp: $ts}' > latest.json

          # Build analysis_digest.json (root)
          python - <<'PY'
          import os, json, pandas as pd
          dd = os.environ["TODAY"]
          over = f"data/{dd}/overlay_vwap_macd_rsi.csv"
          opl  = f"data/{dd}/option_pl.csv"
          gap  = f"data/{dd}/gapdown_above_100sma.csv"
          out = "analysis_digest.json"
          digest = {"date_dir": f"data/{dd}"}
          def preview(path, cols=None, n=30):
            if not os.path.exists(path): return None
            df = pd.read_csv(path)
            if cols: df = df[[c for c in cols if c in df.columns]]
            return df.head(n).to_dict(orient="records")
          digest["overlay_preview"] = preview(
            over,
            ["Ticker","RSI14","MACD>Signal","VWAP","LastPx","Px_vs_VWAP","SMA100","Gap%","Guidance"], 30
          )
          digest["option_pl_rows"] = (pd.read_csv(opl).shape[0] if os.path.exists(opl) else 0)
          digest["gap_rows"]       = (pd.read_csv(gap).shape[0] if os.path.exists(gap) else 0)
          with open(out,"w") as f: json.dump(digest,f,indent=2)
          print("Wrote", out)
          PY

          echo "ðŸ“¦ Collected into $DD" >> "$GITHUB_STEP_SUMMARY"
          ls -lh "$DD" >> "$GITHUB_STEP_SUMMARY"

      - name: Commit artifacts
        if: steps.skipcheck.outputs.already == 'false'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "LEAPS daily overlay â€” ${{ needs.initialize.outputs.date_dir }}"
          branch: ${{ env.BRANCH }}
          file_pattern: |
            data/**
            latest.json
            analysis_digest.json
          add_options: -A
          commit_options: "--no-verify"

      - name: Prepare public folder for Pages
        if: steps.skipcheck.outputs.already == 'false'
        env:
          DD: ${{ needs.initialize.outputs.date_dir }}
        run: |
          set -euo pipefail
          rm -rf public && mkdir -p public
          cp -R "data/${DD}" "public/${DD}"
          cp -f latest.json analysis_digest.json public/

      - name: Publish to GitHub Pages
        if: steps.skipcheck.outputs.already == 'false'
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: public
          keep_files: true
          enable_jekyll: false

      - name: Cleanup old data dirs (> ${DATA_RETENTION_DAYS}d)
        continue-on-error: true
        run: |
          [[ -d data ]] && find data -maxdepth 1 -type d -name "20*" -mtime +${DATA_RETENTION_DAYS} -print -exec rm -rf {} + || true

  consumer:
    needs: [initialize, producer]
    if: needs.initialize.outputs.run_ok == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: OS tools + jq
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends curl jq coreutils
          sudo rm -rf /var/lib/apt/lists/*

      - name: Make fetch tool executable
        run: chmod +x tools/fetch.sh

      - name: Resolve effective date_dir (prefer latest.json â‰¤24h; else newest; else yesterday)
        id: datedir
        shell: bash
        run: |
          set -euo pipefail
          TODAY="$(date -u +%Y-%m-%d)"
          CAND=""

          if [[ -s latest.json ]]; then
            ptr="$(jq -r '.date_dir // empty' latest.json | sed 's|data/||')"
            if [[ -n "$ptr" ]]; then
              age_h=$(( ( $(date -u +%s) - $(date -u -d "$ptr" +%s) ) / 3600 ))
              if (( age_h >= 0 && age_h <= 24 )); then CAND="$ptr"; fi
            fi
          fi
          if [[ -z "$CAND" && -d data ]]; then
            CAND="$(find data -maxdepth 1 -type d -name "20*" | sort -r | head -1 | xargs -I{} basename {} || true)"
          fi
          if [[ -z "$CAND" ]]; then
            CAND="$(date -u -d "yesterday" +%Y-%m-%d)"
          fi

          echo "date_dir=$CAND" >> "$GITHUB_OUTPUT"
          echo "Using date_dir=$CAND" >> "$GITHUB_STEP_SUMMARY"

      - name: Fetch overlay/PL/gap (Pages â†’ jsDelivr â†’ raw â†’ API)
        env:
          DD: ${{ steps.datedir.outputs.date_dir }}
          OWNER: ${{ env.OWNER }}
          REPO:  ${{ env.REPO }}
          BRANCH: ${{ env.BRANCH }}
        shell: bash
        run: |
          set -euo pipefail
          tools/fetch.sh "data/${DD}/overlay_vwap_macd_rsi.csv" overlay.csv \
                         "data/${DD}/option_pl.csv"            option_pl.csv \
                         "data/${DD}/gapdown_above_100sma.csv" gap.csv

      - name: Emit quick summary
        run: |
          {
            echo "### ðŸ“Š Consumer Snapshot"
            echo "**date_dir:** ${{ steps.datedir.outputs.date_dir }}"
            for f in overlay.csv option_pl.csv gap.csv; do
              if [[ -f "$f" ]]; then
                echo ""
                echo "#### $f (head)"
                echo '```csv'
                head -5 "$f"
                echo '```'
              fi
            done
          } >> "$GITHUB_STEP_SUMMARY"

  notify:
    needs: [initialize, producer, consumer]
    if: always()
    runs-on: ubuntu-latest
    steps:
      - name: Final status
        run: |
          echo "init=${{ needs.initialize.result }}, prod=${{ needs.producer.result }}, cons=${{ needs.consumer.result }}"
