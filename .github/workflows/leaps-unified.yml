name: LEAPS Unified (produce + consume)

on:
  # 10:50 AM local PT (DST-aware, weekdays only)
  # PDT (Mar–Oct) 17:50 UTC
  schedule:
    - cron: '50 17 * 3-10 1-5'
    # PST (Nov–Feb) 18:50 UTC
    - cron: '50 18 * 11,12,1,2 1-5'
  workflow_dispatch:

permissions:
  contents: write   # commit output + publish gh-pages
  pages: write

env:
  PYTHON_VERSION: "3.11"
  DESIRED_PT_TIME: "10:50"   # target local PT fire time
  WINDOW_MIN: "600"           # ± minutes allowed around target (keeps single daily run)
  REPO: "Sevenon7/Tradier_Options"
  BRANCH: "main"
  OWNER: "Sevenon7"

jobs:
  # =========================
  # 1) TIME GATE (PT window)
  # =========================
  time-gate:
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      run_ok: ${{ steps.tgate.outputs.run_ok }}
    steps:
      - name: Time Gate (America/Los_Angeles)
        id: tgate
        shell: bash
        run: |
          set -euo pipefail
          export TZ=America/Los_Angeles
          NOW_PT="$(date +%H:%M)"
          TARGET_PT="${DESIRED_PT_TIME}"

          # Compute |Δ| minutes between NOW_PT and TARGET_PT using GNU date
          now_s="$(date -d "$NOW_PT" +%s)"
          tgt_s="$(date -d "$TARGET_PT" +%s)"
          diff="$(( now_s - tgt_s ))"; [ "$diff" -lt 0 ] && diff="$(( -diff ))"
          DELTA="$(( diff / 60 ))"

          echo "Now PT: ${NOW_PT}" >> "$GITHUB_STEP_SUMMARY"
          echo "Target PT: ${TARGET_PT}" >> "$GITHUB_STEP_SUMMARY"
          echo "Delta (min): ${DELTA}" >> "$GITHUB_STEP_SUMMARY"
          echo "Window (±min): ${WINDOW_MIN}" >> "$GITHUB_STEP_SUMMARY"

          if [ "${DELTA}" -le "${WINDOW_MIN}" ]; then
            echo "run_ok=true"  >> "$GITHUB_OUTPUT"
            echo "✅ Within window — proceeding." >> "$GITHUB_STEP_SUMMARY"
          else
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            echo "⏸️ Outside window — skipping execution." >> "$GITHUB_STEP_SUMMARY"
          fi

  # =========================
  # 2) PRODUCER
  # =========================
  producer:
    env:
        PYTHONPATH: ${{ github.workspace }}
    needs: time-gate
    if: needs.time-gate.outputs.run_ok == 'true'
    runs-on: ubuntu-latest
    timeout-minutes: 25
    outputs:
      date_dir: ${{ steps.collect.outputs.date_dir }}
      already_ran: ${{ steps.skipcheck.outputs.already }}
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Ensure OS tools (curl/jq)
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y curl jq coreutils

      - name: Create requirements.txt (deterministic)
        run: |
          set -euo pipefail
          printf '%s\n' \
            'requests' \
            'pandas' \
            'numpy' \
            'python-dateutil' > requirements.txt
          echo "----- requirements.txt -----"
          cat requirements.txt

      - name: Install deps
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Mask token (log hygiene)
        run: echo "::add-mask::${{ secrets.TRADIER_TOKEN }}"

      - name: Skip if already ran today (UTC)
        id: skipcheck
        shell: bash
        run: |
          set -euo pipefail
          DATE_DIR="data/$(date -u +%Y-%m-%d)"
          if [ -d "$DATE_DIR" ]; then
            echo "already=true"  >> "$GITHUB_OUTPUT"
            echo "⏭️ Producer already completed for $DATE_DIR" >> "$GITHUB_STEP_SUMMARY"
          else
            echo "already=false" >> "$GITHUB_OUTPUT"
            echo "✨ First run for $DATE_DIR — executing Producer." >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Run overlay script (leaps_batched_cached.py)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          set -euo pipefail
          python leaps_batched_cached.py

      - name: Build Actual Option P/L (tools/option_pl_builder.py)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          if [ -f tools/option_pl_builder.py ]; then
            python tools/option_pl_builder.py
          else
            echo "⚠️ tools/option_pl_builder.py not present; skipping." >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Enrich overlay with intraday VWAP (optional)
        if: steps.skipcheck.outputs.already == 'false'
        continue-on-error: true
        run: |
          if [ -f tools/enrich_overlay_with_vwap.py ]; then
            # Run as a package so "from tools.*" works in any CWD
            if python -m tools.enrich_overlay_with_vwap --overlay overlay_vwap_macd_rsi.csv; then
              echo "✅ VWAP enrichment completed" >> "$GITHUB_STEP_SUMMARY"
            else
              echo "::warning::VWAP enrichment failed (non-blocking)"
              echo "⚠️ VWAP enrichment failed (non-critical)" >> "$GITHUB_STEP_SUMMARY"
            fi
          else
            echo "::warning::tools/enrich_overlay_with_vwap.py not found"
          fi

      - name: Collect artifacts into date dir + update latest.json
        if: steps.skipcheck.outputs.already == 'false'
        id: collect
        shell: bash
        run: |
          set -euo pipefail
          TODAY="$(date -u +%Y-%m-%d)"
          DD="data/${TODAY}"
          mkdir -p "$DD"

          # Move any root-level outputs into the date folder
          for f in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv vwap_missing.json; do
            [ -f "$f" ] && mv -f "$f" "$DD/"
          done

          # Refresh latest.json pointer at repo root
          echo '{"date_dir":"data/'"${TODAY}"'"}' > latest.json

          echo "date_dir=${TODAY}" >> "$GITHUB_OUTPUT"

          {
            echo "### 📦 Producer Artifacts"
            echo "- **Directory:** \`${DD}\`"
            echo "- **Contents:**"
            ls -lh "$DD" | tail -n +2 | awk '{printf "  - `%s` (%s)\n", $9, $5}'
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Build analysis_digest.json (root + date_dir)
        if: steps.skipcheck.outputs.already == 'false'
        env:
          DATE_ONLY: ${{ needs.initialize.outputs.date_dir }}   # e.g. 2025-10-08
        shell: bash
        run: |
          set -euo pipefail

          BASE_DIR="data/${DATE_ONLY}"
          OVERLAY="${BASE_DIR}/overlay_vwap_macd_rsi.csv"
          OPTPL="${BASE_DIR}/option_pl.csv"
          GAP="${BASE_DIR}/gapdown_above_100sma.csv"

          if [ ! -f "${OVERLAY}" ]; then
            echo "::error ::overlay CSV not found at ${OVERLAY}"
            exit 1
          fi

          python - <<'PY'
          import os, json, datetime, pandas as pd
          
          date_only = os.environ["DATE_ONLY"]
          base = os.path.join("data", date_only)
          
          overlay = os.path.join(base, "overlay_vwap_macd_rsi.csv")
          optpl   = os.path.join(base, "option_pl.csv")
          gap     = os.path.join(base, "gapdown_above_100sma.csv")
          
          out = {
              "generated_utc": datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
              "raw_links": {
                  "overlay": f"https://raw.githubusercontent.com/Sevenon7/Tradier_Options/main/{overlay}",
                  "option_pl": f"https://raw.githubusercontent.com/Sevenon7/Tradier_Options/main/{optpl}",
                  "gap_screen": f"https://raw.githubusercontent.com/Sevenon7/Tradier_Options/main/{gap}",
                  "ready": f"https://raw.githubusercontent.com/Sevenon7/Tradier_Options/main/{base}/READY",
                  "latest": "https://raw.githubusercontent.com/Sevenon7/Tradier_Options/main/latest.json",
              },
              "overlay_rows": 0,
              "option_pl_rows": 0,
              "gap_rows": 0,
          }
          
          try:
              out["overlay_rows"] = int(pd.read_csv(overlay).shape[0])
          except Exception as e:
              out["overlay_error"] = str(e)
          
          if os.path.exists(optpl):
              try:
                  out["option_pl_rows"] = int(pd.read_csv(optpl).shape[0])
              except Exception as e:
                  out["option_pl_error"] = str(e)
          
          if os.path.exists(gap):
              try:
                  out["gap_rows"] = int(pd.read_csv(gap).shape[0])
              except Exception as e:
                  out["gap_error"] = str(e)
          
          # save to date dir and root
          with open(os.path.join(base, "analysis_digest.json"), "w") as f:
              json.dump(out, f, indent=2)
          
          with open("analysis_digest.json", "w") as f:
              json.dump(out, f, indent=2)
          PY

      - name: Commit daily artifacts to repo
        if: steps.skipcheck.outputs.already == 'false'
        uses: stefanzweifel/git-auto-commit-action@v5
        with:
          commit_message: "📊 LEAPS daily overlay - ${{ needs.initialize.outputs.date_dir }}"
          branch: ${{ env.BRANCH }}
          add_options: -A
          commit_options: '--no-verify'
          file_pattern: 'data/* latest.json analysis_digest.json'

      - name: Publish CSV mirror to gh-pages (static backup)
        if: steps.skipcheck.outputs.already == 'false'
        uses: peaceiris/actions-gh-pages@v3
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: data/${{ steps.collect.outputs.date_dir }}
          destination_dir: data/${{ steps.collect.outputs.date_dir }}
          keep_files: true

  # =========================
  # 3) CONSUMER
  # =========================
  consumer:
    needs: [time-gate, producer]
    if: needs.time-gate.outputs.run_ok == 'true' && !cancelled()
    runs-on: ubuntu-latest
    timeout-minutes: 10
    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Ensure OS tools
        run: |
          set -euo pipefail
          sudo apt-get update -y
          sudo apt-get install -y curl jq coreutils

      - name: Make fetch tool executable (repo path)
        run: chmod +x tools/fetch.sh

      - name: Resolve effective date_dir (prefer local latest.json; else newest ≤24h; else yesterday)
        id: datedir
        shell: bash
        run: |
          set -euo pipefail
          TODAY="$(date -u +%Y-%m-%d)"
          DATE_DIR=""

          # 1) local latest.json (if present)
          if [ -f latest.json ]; then
            ptr="$(jq -r '.date_dir // empty' latest.json | sed 's|^data/||')"
            if [ -n "$ptr" ]; then
              DATE_DIR="$ptr"
              echo "Using local pointer: $DATE_DIR"
            fi
          fi

          # 2) newest folder in /data (if pointer missing)
          if [ -z "$DATE_DIR" ] && [ -d data ]; then
            DATE_DIR="$(find data -maxdepth 1 -type d -name '20*' | sort | tail -1 | sed 's|data/||')"
            [ -n "$DATE_DIR" ] && echo "Using newest data folder: $DATE_DIR"
          fi

          # 3) ≤24h guard, else fallback to yesterday
          if [ -n "$DATE_DIR" ]; then
            py_age="$(python - <<'PY'
            import os,sys,datetime
            d=os.environ["DATE_DIR"]
            dt=datetime.datetime.strptime(d, "%Y-%m-%d")
            print(int((datetime.datetime.utcnow()-dt).total_seconds()/3600))
            PY
            )"
            if [ "$py_age" -gt 24 ]; then
              DATE_DIR="$(date -u -d 'yesterday' +%Y-%m-%d)"
              echo "Pointer too old; fallback to yesterday: $DATE_DIR"
            fi
          else
            DATE_DIR="$(date -u -d 'yesterday' +%Y-%m-%d)"
            echo "No pointer / dirs; fallback to yesterday: $DATE_DIR"
          fi

          echo "date_dir=${DATE_DIR}" >> "$GITHUB_OUTPUT"
          echo "📁 Resolved data directory: data/${DATE_DIR}" >> "$GITHUB_STEP_SUMMARY"

      - name: Fetch overlay/PL/gap with fallback chain (raw → API → Pages)
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ env.REPO }}
          BRANCH: ${{ env.BRANCH }}
        shell: bash
        run: |
          set -euo pipefail
          DD="${{ steps.datedir.outputs.date_dir }}"
          tools/fetch.sh \
            "data/${DD}/overlay_vwap_macd_rsi.csv" overlay.csv \
            "data/${DD}/option_pl.csv"            option_pl.csv \
            "data/${DD}/gapdown_above_100sma.csv" gap.csv

      - name: Emit quick summary to Job Summary
        run: |
          {
            echo "### 📊 Consumer Summary"
            echo "- **date_dir:** ${{ steps.datedir.outputs.date_dir }}"
            echo ""
            if [ -f overlay.csv ]; then
              echo "**overlay.csv (head):**"
              head -5 overlay.csv
              echo ""
            else
              echo "⚠️ overlay.csv not found"
            fi
            if [ -f option_pl.csv ]; then
              echo "**option_pl.csv (head):**"
              head -5 option_pl.csv
              echo ""
            else
              echo "⚠️ option_pl.csv not found"
            fi
            if [ -f gap.csv ]; then
              echo "**gapdown_above_100sma.csv (head):**"
              head -5 gap.csv
              echo ""
            else
              echo "⚠️ gapdown_above_100sma.csv not found"
            fi
          } >> "$GITHUB_STEP_SUMMARY"
