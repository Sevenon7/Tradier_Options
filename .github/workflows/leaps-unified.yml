name: LEAPS Unified (produce + consume)

on:
  # One UTC schedule; time gate limits execution to 10:50 PT Â± WINDOW_MIN on weekdays
  schedule:
    - cron: "50 17,18 * * 1-5"
  workflow_dispatch:
    inputs:
      skip_time_gate:
        description: "Skip time gate (manual runs only)"
        required: false
        default: "false"
        type: choice
        options: ["false","true"]
      force_run:
        description: "Force producer even if today's data exists"
        required: false
        default: "false"
        type: choice
        options: ["false","true"]
      target_date:
        description: "Target YYYY-MM-DD (optional)"
        required: false
        type: string
      debug_mode:
        description: "Enable extra logging"
        required: false
        default: "false"
        type: choice
        options: ["false","true"]

concurrency:
  group: leaps-unified-${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: read   # keep global minimal; elevate per job

env:
  PYTHON_VERSION: "3.11"
  DESIRED_PT_TIME: "10:50"     # local PT target time
  WINDOW_MIN: "95"             # Â± window (mins) to allow exactly one daily run
  REPO: "Sevenon7/Tradier_Options"
  BRANCH: "main"
  OWNER: "Sevenon7"
  DATA_RETENTION_DAYS: "30"

defaults:
  run:
    shell: bash

jobs:
  # ============================================
  # 1) TIME GATE & INIT
  # ============================================
  init:
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    permissions:
      contents: read
    outputs:
      run_ok: ${{ steps.tgate.outputs.run_ok }}
      date_dir: ${{ steps.setdate.outputs.date_dir }}
    steps:
      - name: Validate required secrets
        run: |
          [[ -n "${{ secrets.TRADIER_TOKEN }}" ]] || { echo "::error::TRADIER_TOKEN not configured"; exit 1; }
          [[ -n "${{ secrets.GITHUB_TOKEN }}"  ]] || { echo "::error::GITHUB_TOKEN not available"; exit 1; }

      - name: Mask tokens early
        run: |
          echo "::add-mask::${{ secrets.TRADIER_TOKEN }}"
          echo "::add-mask::${{ secrets.GITHUB_TOKEN }}"

      - name: Resolve target date (UTC)
        id: setdate
        run: |
          set -euo pipefail
          if [[ -n "${{ github.event.inputs.target_date || '' }}" ]]; then
            DD="${{ github.event.inputs.target_date }}"
            date -d "$DD" +%Y-%m-%d >/dev/null 2>&1 || { echo "::error::Invalid date: $DD"; exit 1; }
          else
            DD="$(date -u +%Y-%m-%d)"
          fi
          echo "date_dir=${DD}" >> "$GITHUB_OUTPUT"
          {
            echo "### ðŸš€ Init"
            echo "- **Target Date (UTC):** \`${DD}\`"
            echo "- **Trigger:** ${{ github.event_name }}"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Time Gate (America/Los_Angeles)
        id: tgate
        run: |
          set -euo pipefail
          if [[ "${{ github.event_name }}" == "workflow_dispatch" && "${{ github.event.inputs.skip_time_gate || 'false' }}" == "true" ]]; then
            echo "run_ok=true" >> "$GITHUB_OUTPUT"
            echo "â­ï¸ Time gate skipped (manual override)." >> "$GITHUB_STEP_SUMMARY"
            exit 0
          fi

          export TZ=America/Los_Angeles
          NOW_PT="$(date +%H:%M)"
          TARGET_PT="${DESIRED_PT_TIME}"
          DOW="$(date +%u)" # 1..7 (Mon..Sun)

          if [[ "$DOW" -gt 5 ]]; then
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            echo "â¸ï¸ Weekend â€” skipping." >> "$GITHUB_STEP_SUMMARY"
            exit 0
          fi

          now_s="$(date -d "$NOW_PT" +%s)"
          tgt_s="$(date -d "$TARGET_PT" +%s)"
          diff="$(( now_s - tgt_s ))"; [[ $diff -lt 0 ]] && diff="$(( -diff ))"
          delta="$(( diff / 60 ))"

          {
            echo "### â° Time Gate"
            echo "- Now (PT): ${NOW_PT}"
            echo "- Target (PT): ${TARGET_PT}"
            echo "- Î” (min): ${delta} | Window: Â±${WINDOW_MIN}"
          } >> "$GITHUB_STEP_SUMMARY"

          if [[ "$delta" -le "$WINDOW_MIN" ]]; then
            echo "run_ok=true" >> "$GITHUB_OUTPUT"
            echo "âœ… Within window â€” continue." >> "$GITHUB_STEP_SUMMARY"
          else
            echo "run_ok=false" >> "$GITHUB_OUTPUT"
            echo "â¸ï¸ Outside window â€” skip." >> "$GITHUB_STEP_SUMMARY"

  # ============================================
  # 2) PRODUCER
  # ============================================
  producer:
    needs: init
    if: needs.init.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    timeout-minutes: 30
    permissions:
      contents: write
    outputs:
      out_dir: ${{ steps.collect.outputs.date_dir }}
      already: ${{ steps.skip.outputs.already }}
    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11  # v4.1.1
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@0a5c61591373683505ea898e09a3ea4f39ef2b9c # v5.0.0
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Create deterministic requirements.txt
        run: |
          cat > requirements.txt <<'REQ'
          requests==2.32.3
          pandas==2.2.2
          numpy==1.26.4
          python-dateutil==2.9.0.post0
          REQ
          echo "### ðŸ“¦ Requirements" >> "$GITHUB_STEP_SUMMARY"
          nl -ba requirements.txt >> "$GITHUB_STEP_SUMMARY"

      - name: Install OS + Python deps
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends curl jq coreutils
          python -m pip install --upgrade pip setuptools wheel
          pip install -r requirements.txt

      - name: Verify environment
        run: |
          python - <<'PY'
          import pandas, requests, numpy, dateutil
          print("âœ… Deps OK:", pandas.__version__, requests.__version__, numpy.__version__, dateutil.__version__)
          PY

      - name: Skip if already ran (UTC)
        id: skip
        run: |
          set -euo pipefail
          DD="${{ needs.init.outputs.date_dir }}"
          if [[ "${{ github.event.inputs.force_run || 'false' }}" == "true" ]]; then
            echo "already=false" >> "$GITHUB_OUTPUT"
            echo "ðŸ”„ Force run enabled." >> "$GITHUB_STEP_SUMMARY"
          elif [[ -s "data/${DD}/overlay_vwap_macd_rsi.csv" ]]; then
            echo "already=true" >> "$GITHUB_OUTPUT"
            echo "â­ï¸ Producer already completed for \`data/${DD}\`." >> "$GITHUB_STEP_SUMMARY"
          else
            echo "already=false" >> "$GITHUB_OUTPUT"
            echo "âœ¨ Producer running for \`data/${DD}\`." >> "$GITHUB_STEP_SUMMARY"
          fi

      - name: Run overlay script
        if: steps.skip.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          set -euo pipefail
          python leaps_batched_cached.py

      - name: Build Actual Option P/L (optional)
        if: steps.skip.outputs.already == 'false'
        env:
          TRADIER_TOKEN: ${{ secrets.TRADIER_TOKEN }}
        run: |
          if [[ -f tools/option_pl_builder.py ]]; then
            python tools/option_pl_builder.py || echo "::warning::option_pl_builder.py failed (non-critical)"
          fi

      - name: Enrich overlay with intraday VWAP (optional)
        if: steps.skip.outputs.already == 'false'
        env:
          PYTHONPATH: ${{ github.workspace }}   # fix ModuleNotFoundError: tools
        run: |
          if [[ -f tools/enrich_overlay_with_vwap.py ]]; then
            python tools/enrich_overlay_with_vwap.py --overlay overlay_vwap_macd_rsi.csv \
              || echo "::warning::VWAP enrichment failed (non-critical)"
          fi

      - name: Collect artifacts into date folder (guard: overlay must exist)
        if: steps.skip.outputs.already == 'false'
        id: collect
        run: |
          set -euo pipefail
          DD="${{ needs.init.outputs.date_dir }}"
          DEST="data/${DD}"
          mkdir -p "$DEST"
          [[ -s overlay_vwap_macd_rsi.csv ]] || { echo "::error::overlay_vwap_macd_rsi.csv missing; abort"; exit 1; }
          moved=0
          for f in overlay_vwap_macd_rsi.csv option_pl.csv gapdown_above_100sma.csv vwap_missing.json; do
            if [[ -f "$f" ]]; then mv -f "$f" "$DEST/"; moved=$((moved+1)); fi
          done
          echo "date_dir=${DD}" >> "$GITHUB_OUTPUT"
          echo "ðŸ“¦ Moved ${moved} files to ${DEST}" >> "$GITHUB_STEP_SUMMARY"

      - name: Sync main (ff-only or rebase) before commit
        if: steps.skip.outputs.already == 'false'
        run: |
          set -euo pipefail
          git fetch origin main
          (git merge --ff-only origin/main) || (git rebase origin/main || true)

      - name: Commit data dir
        if: steps.skip.outputs.already == 'false'
        uses: stefanzweifel/git-auto-commit-action@8621497c8c39c72f3e2a999a26b4ca1b5058a842 # v5
        with:
          commit_message: "LEAPS: data for ${{ steps.collect.outputs.date_dir }}"
          add_options: "-A"
          file_pattern: |
            data/${{ steps.collect.outputs.date_dir }}/*

  # ============================================
  # 3) PUBLISH (latest.json + analysis_digest.json + Pages)
  # ============================================
  publish:
    needs: [init, producer]
    if: needs.init.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    timeout-minutes: 10
    permissions:
      contents: write
      pages: write
      id-token: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Install system deps (jq)
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends jq coreutils

      - name: Determine effective data_dir
        id: datedir
        shell: bash
        run: |
          set -euo pipefail
          CAND=""
          # Prefer pointer if it exists & points to an existing overlay
          if [[ -s latest.json ]]; then
            ptr="$(jq -r '.date_dir // empty' latest.json | sed 's|^data/||')"
            [[ -n "$ptr" && -s "data/$ptr/overlay_vwap_macd_rsi.csv" ]] && CAND="$ptr"
          fi
          # Otherwise pick newest data dir that has overlay
          if [[ -z "$CAND" && -d data ]]; then
            CAND="$(find data -maxdepth 1 -type d -name '20*' \
                     -exec test -s '{}/overlay_vwap_macd_rsi.csv' ';' -print \
                     | sed 's|^data/||' | sort | tail -1 || true)"
          fi
          if [[ -z "$CAND" ]]; then
            echo "::error::No valid data directory found with overlay CSV"
            exit 1
          fi
          echo "dd=$CAND" >> "$GITHUB_OUTPUT"
          echo "âœ… Using data dir: $CAND" >> "$GITHUB_STEP_SUMMARY"

      - name: Setup Python (for digest)
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install tiny deps (digest)
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip >/dev/null
          pip install pandas==2.2.2 python-dateutil==2.9.0.post0 >/dev/null

      - name: Build latest.json + analysis_digest.json
        id: build_digest
        env:
          DD: ${{ steps.datedir.outputs.dd }}
        shell: bash
        run: |
          set -euo pipefail
          # Pointer (write only after dd is confirmed valid)
          jq -n \
            --arg dd "data/${DD}" \
            --arg ts "$(date -u +%Y-%m-%dT%H:%M:%SZ)" \
            '{date_dir:$dd, generated_utc:$ts}' > latest.json

          # Digest
          python - <<'PY'
          import os, json, pandas as pd
          dd = os.environ["DD"]
          base = f"data/{dd}"

          def meta(path, preview_cols=None, n=20):
              m = {"status":"missing"}
              if not os.path.exists(path): return m
              try:
                  df = pd.read_csv(path)
                  m = {"status":"ok","rows":int(len(df)),"columns":int(len(df.columns))}
                  if preview_cols:
                      cols = [c for c in preview_cols if c in df.columns]
                      if cols:
                          m["preview"] = df[cols].head(n).to_dict(orient="records")
                  return m
              except Exception as e:
                  return {"status":"error","error":str(e)}

          digest = {
            "date_dir": f"data/{dd}",
            "generated_utc": pd.Timestamp.utcnow().isoformat()+"Z",
            "files": {
              "overlay": meta(f"{base}/overlay_vwap_macd_rsi.csv",
                ["Ticker","RSI14","MACD>Signal","VWAP","LastPx","Px_vs_VWAP","SMA100","Gap%","Guidance"]),
              "option_pl": meta(f"{base}/option_pl.csv"),
              "gap":      meta(f"{base}/gapdown_above_100sma.csv")
            }
          }
          with open("analysis_digest.json","w") as f: json.dump(digest,f,indent=2)
          PY

          echo "ðŸ“„ Built latest.json and analysis_digest.json" >> "$GITHUB_STEP_SUMMARY"

      - name: Commit pointers + digest (rebase-safe)
        env:
          BRANCH: ${{ github.ref_name }}
        shell: bash
        run: |
          set -euo pipefail
          git config user.name  "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          git add latest.json analysis_digest.json
          git commit -m "LEAPS: refresh pointers & digest (data/${{ steps.datedir.outputs.dd }})" || echo "no changes"

          tries=0
          until [ $tries -ge 3 ]
          do
            git fetch origin "$BRANCH"
            if git rebase origin/"$BRANCH"; then
              if git push origin HEAD:"$BRANCH"; then
                echo "âœ… Pushed after rebase"; exit 0
              fi
            else
              git rebase --abort || true
              git merge --no-edit origin/"$BRANCH" || true
              if git push origin HEAD:"$BRANCH"; then
                echo "âœ… Pushed after merge"; exit 0
              fi
            fi
            tries=$((tries+1))
            echo "â³ Retry $triesâ€¦" ; sleep $((5+5*tries))
          done

          echo "::warning::Falling back to --force-with-lease"
          git push --force-with-lease origin HEAD:"$BRANCH"

      - name: Verify artifacts present
        shell: bash
        run: |
          test -s latest.json || (echo "::error::latest.json missing" && exit 1)
          test -s analysis_digest.json || (echo "::error::analysis_digest.json missing" && exit 1)
          echo "âœ… Verified non-empty pointers"

      - name: Prepare GitHub Pages payload
        shell: bash
        run: |
          set -euo pipefail
          rm -rf pages_pub && mkdir -p pages_pub/data
          cp -a latest.json analysis_digest.json pages_pub/
          cp -a "data/${{ steps.datedir.outputs.dd }}" "pages_pub/data/${{ steps.datedir.outputs.dd }}"

      - name: Publish to GitHub Pages
        uses: peaceiris/actions-gh-pages@v4
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          publish_dir: pages_pub
          keep_files: true
          enable_jekyll: false

  # ============================================
  # 4) CONSUMER
  # ============================================
  consumer:
    needs: [init, publish]
    if: needs.init.outputs.run_ok == 'true'
    runs-on: ubuntu-24.04
    timeout-minutes: 12
    permissions:
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11

      - name: Install minimal tools
        run: |
          sudo apt-get update -qq
          sudo apt-get install -y --no-install-recommends curl jq coreutils

      - name: Ensure tools/fetch.sh
        run: |
          if [[ -f tools/fetch.sh ]]; then
            chmod +x tools/fetch.sh
          else
            echo "::error::tools/fetch.sh missing"; exit 1;
          fi

      - name: Resolve effective date_dir for consumer
        id: datedir
        run: |
          set -euo pipefail
          CAND=""
          if [[ -s latest.json ]]; then
            ptr="$(jq -r '.date_dir // empty' latest.json | sed 's|^data/||')"
            [[ -n "$ptr" && -s "data/${ptr}/overlay_vwap_macd_rsi.csv" ]] && CAND="$ptr"
          fi
          if [[ -z "$CAND" && -d data ]]; then
            CAND="$(find data -maxdepth 1 -type d -name '20*' \
                     -exec test -s '{}/overlay_vwap_macd_rsi.csv' ';' -print \
                     | sed 's|^data/||' | sort | tail -1 || true)"
          fi
          if [[ -z "$CAND" ]]; then
            echo "::error::No valid data directory with overlay for consumer"; exit 1
          fi
          echo "date_dir=${CAND}" >> "$GITHUB_OUTPUT"
          echo "### ðŸ“… Consumer using: ${CAND}" >> "$GITHUB_STEP_SUMMARY"

      - name: Fetch overlay/PL/gap (Pages â†’ jsDelivr â†’ raw)
        env:
          DD: ${{ steps.datedir.outputs.date_dir }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          OWNER: ${{ env.OWNER }}
          REPO: ${{ env.REPO }}
          BRANCH: ${{ env.BRANCH }}
        run: |
          set -euo pipefail
          found=0
          if [[ -s "data/${DD}/overlay_vwap_macd_rsi.csv" ]]; then cp "data/${DD}/overlay_vwap_macd_rsi.csv" overlay.csv; found=$((found+1)); fi
          if [[ -s "data/${DD}/option_pl.csv" ]]; then cp "data/${DD}/option_pl.csv" option_pl.csv; fi
          if [[ -s "data/${DD}/gapdown_above_100sma.csv" ]]; then cp "data/${DD}/gapdown_above_100sma.csv" gap.csv; fi

          if [[ $found -eq 0 ]]; then
            tools/fetch.sh \
              "data/${DD}/overlay_vwap_macd_rsi.csv" overlay.csv \
              "data/${DD}/option_pl.csv" option_pl.csv \
              "data/${DD}/gapdown_above_100sma.csv" gap.csv
          fi
          test -s overlay.csv || { echo "::error::overlay.csv missing after fetch"; exit 1; }

      - name: Consumer summary
        run: |
          {
            echo "### ðŸ“Š Consumer Data"
            for f in overlay.csv option_pl.csv gap.csv; do
              if [[ -s "$f" ]]; then
                echo "#### $f"
                echo '```csv'; head -5 "$f"; echo '```'
              fi
            done
          } >> "$GITHUB_STEP_SUMMARY"

  # ============================================
  # 5) NOTIFY
  # ============================================
  notify:
    needs: [init, producer, publish, consumer]
    if: always()
    runs-on: ubuntu-24.04
    timeout-minutes: 5
    permissions:
      contents: read
      issues: write
    steps:
      - name: Final status
        id: status
        run: |
          ok_init="${{ needs.init.result }}"
          ok_prod="${{ needs.producer.result }}"
          ok_pub="${{ needs.publish.result }}"
          ok_cons="${{ needs.consumer.result }}"

          overall="success"
          for r in "$ok_init" "$ok_prod" "$ok_pub" "$ok_cons"; do
            [[ "$r" == "failure" ]] && overall="failure"
          done
          echo "overall=${overall}" >> "$GITHUB_OUTPUT"

          {
            echo "## ðŸ“‹ Workflow Summary"
            echo "| Job | Result |"
            echo "|-----|--------|"
            echo "| init | $ok_init |"
            echo "| producer | $ok_prod |"
            echo "| publish | $ok_pub |"
            echo "| consumer | $ok_cons |"
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Open issue on failure
        if: steps.status.outputs.overall == 'failure' && github.event_name != 'workflow_dispatch'
        uses: actions/github-script@v7
        with:
          script: |
            const date = new Date().toISOString().slice(0,10);
            const runUrl = `${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}`;
            await github.rest.issues.create({
              owner: context.repo.owner,
              repo: context.repo.repo,
              title: `Workflow Failure: LEAPS Unified - ${date}`,
              body: `Run: ${runUrl}\n\nPlease investigate the failing job(s).`,
              labels: ['workflow-failure','automated']
            })
